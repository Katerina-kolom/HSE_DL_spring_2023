{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOOLQMEgt6KJ"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/NktBkzn/HSE_DL_2021/blob/master/12_week/RNNs.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nje_LngItrLy"
      },
      "source": [
        "- Ноутбук заимствован с курса [dlcourse.ai](https://dlcourse.ai)\n",
        "- Решение ноутбука взято [отсюда](https://github.com/omega1996/dlcourse/blob/master/assignments/assignment6/RNNs.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiPbD_lTXUMd"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P59NYU98GCb9"
      },
      "source": [
        "!pip install gensim==4.1.2\n",
        "!pip install nltk==3.6.3\n",
        "!pip install scikit-learn==1.0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TvJm34-fd5_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9c51274-a93e-49c2-d0b6-cf641da317fb"
      },
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.6.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sVtGHmA9aBM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiA2dGmgF1rW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226d5a73-487b-4c05-f15d-e3d262e83bad"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTNDocatrWAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62d0503b-09b1-48de-ff48-bf9f75a05d94"
      },
      "source": [
        "len(data), len(data[0]), len(data[1])\n",
        "\n",
        "# list(list(tuple))\n",
        "\n",
        "# (word, tag)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(57340, 25, 43)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcAN6X_Vn5--",
        "outputId": "42dad741-799b-41ac-c0f6-f1d6e0572ab6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DET'),\n",
              " ('Fulton', 'NOUN'),\n",
              " ('County', 'NOUN'),\n",
              " ('Grand', 'ADJ'),\n",
              " ('Jury', 'NOUN'),\n",
              " ('said', 'VERB'),\n",
              " ('Friday', 'NOUN'),\n",
              " ('an', 'DET'),\n",
              " ('investigation', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " (\"Atlanta's\", 'NOUN'),\n",
              " ('recent', 'ADJ'),\n",
              " ('primary', 'NOUN'),\n",
              " ('election', 'NOUN'),\n",
              " ('produced', 'VERB'),\n",
              " ('``', '.'),\n",
              " ('no', 'DET'),\n",
              " ('evidence', 'NOUN'),\n",
              " (\"''\", '.'),\n",
              " ('that', 'ADP'),\n",
              " ('any', 'DET'),\n",
              " ('irregularities', 'NOUN'),\n",
              " ('took', 'VERB'),\n",
              " ('place', 'NOUN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QstS4NO0L97c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6088585-dabe-42fc-f0ce-da18692ab849"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTai8Ta0lgwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64fb296e-948f-41c8-ddbc-210d9d39964e"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCjwwDs6Zq9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1c86b2-f1e0-408f-aa12-c98a6528e787"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}  # ind + 1 to leave 0 idx blank for <pad>\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in train = 45441. Tags = {'PRON', 'ADJ', 'VERB', 'X', 'CONJ', 'DET', 'ADP', 'ADV', '.', 'PRT', 'NUM', 'NOUN'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(words), type(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtK4WMtUp03E",
        "outputId": "982caeea-7716-403b-b257-b4db24802054"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(set, set)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nGShrtNr59m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d88b580-6cc4-446a-92ca-6f570a077bc6"
      },
      "source": [
        "tag2ind"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PRON': 1,\n",
              " 'ADJ': 2,\n",
              " 'VERB': 3,\n",
              " 'X': 4,\n",
              " 'CONJ': 5,\n",
              " 'DET': 6,\n",
              " 'ADP': 7,\n",
              " 'ADV': 8,\n",
              " '.': 9,\n",
              " 'PRT': 10,\n",
              " 'NUM': 11,\n",
              " 'NOUN': 12,\n",
              " '<pad>': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URC1B2nvPGFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "70abb64b-71d0-473a-9502-eb4b8f43c3a4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MergdmlUcDDz"
      },
      "source": [
        "На вход UnigramTagger принимает данные в таком формате:\\\n",
        "*train (list(list(tuple(str, str)))) – The corpus of training data, a list of tagged sentences*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jihL_L1EcwzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43093af-5bc5-406e-e498-5e7d59b57744"
      },
      "source": [
        "test_data[:2]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Open', 'ADJ'), ('market', 'NOUN'), ('policy', 'NOUN')],\n",
              " [('And', 'CONJ'),\n",
              "  ('you', 'PRON'),\n",
              "  ('think', 'VERB'),\n",
              "  ('you', 'PRON'),\n",
              "  ('have', 'VERB'),\n",
              "  ('language', 'NOUN'),\n",
              "  ('problems', 'NOUN'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rWmSToIaeAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f427d707-c41d-403d-eb22-6524e137bac1"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')  # tags every word witn a noun\n",
        "\n",
        "# The UnigramTagger finds the most likely tag for each word in a training corpus, \n",
        "# and then uses that information to assign tags to new tokens.\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GocIMguUeNKq"
      },
      "source": [
        "Хорошая [заметка](https://stackoverflow.com/questions/46713629/evaluating-pos-tagger-in-nltk) на SO про синтаксис Taggers из nltk\n",
        "\n",
        "Выжимка:\n",
        "```python\n",
        "tagged_sentences = brown.tagged_sents(categories=\"news\", tagset=\"universal\")\n",
        "\n",
        "# let's keep 20% of the data for testing, and 80 for training\n",
        "i = int(len(tagged_sentences)*0.2)\n",
        "train_sentences = tagged_sentences[i:]\n",
        "test_sentences = tagged_sentences[:i]\n",
        "\n",
        "# train\n",
        "unigram_tagger = UnigramTagger(train_sentences)\n",
        "# get ACCURACY; default evaluation metric for nltk taggers is accuracy\n",
        "accuracy = unigram_tagger.evaluate(test_sentences)\n",
        "\n",
        "tagged_test_sentences = unigram_tagger.tag_sents([[token for token,tag in sent] for sent in test_sentences])  # words(tokens) only\n",
        "pred = [str(tag) for sentence in tagged_test_sentences for token,tag in sentence]  # predicted tags\n",
        "gold = [str(tag) for sentence in test_sentences for token,tag in sentence]  # true tags\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(gold, pred))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjz_Rk0bbMyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a1f914-623f-4c19-d0f0-f634212857a0"
      },
      "source": [
        "# A tagger that chooses a token’s tag based its word string and on the preceding \n",
        "# words’ tag. In particular, a tuple consisting of the previous tag and \n",
        "# the word is looked up in a table, and the corresponding tag is returned.\n",
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XCuxEBVbOY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a4c3d2e-e32d-487b-a37c-f965028b6687"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRbIzTcOuSQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e025650-4a74-4d24-98ef-51d2d7e9d0a0"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of trigram tagger = 93.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtRbz1SwgEqc"
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "PNfrMIEGtBmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d760d53a-9ec4-48fb-e5f7-b13705a6a32c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3840,\n",
              " 39349,\n",
              " 19731,\n",
              " 5740,\n",
              " 44855,\n",
              " 13384,\n",
              " 5690,\n",
              " 20281,\n",
              " 37024,\n",
              " 37238,\n",
              " 33012,\n",
              " 34562,\n",
              " 33802,\n",
              " 1765,\n",
              " 25694,\n",
              " 24701,\n",
              " 11223,\n",
              " 13281,\n",
              " 13281]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olEudjozuSFF",
        "outputId": "3268f30d-2fd0-4c2f-8229-523924d7e763"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9, 6, 3, 1, 3, 7, 12, 12, 7, 12, 5, 12, 7, 6, 2, 12, 9, 9, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkGLzl2OuhBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c5d3e3-b329-4260-c4d7-f4861326e690"
      },
      "source": [
        "len(X_train), len(train_data)\n",
        "\n",
        "# len(data[0]), len(data[1])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36554, 36554)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhsTKZalfih6"
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "    # print('n_samples', n_samples)\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        # Создаем матрицы, заполненные нулями, которые ниже будут заполнены значениями\n",
        "        batch_indices = indices[start:end]\n",
        "        # print('batch_indices', batch_indices)\n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        # print('max_sent_len', max_sent_len)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        # заполнение созданных матриц значениями (с учетом того, что все предложения разной длины)\n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4XsRII5kW5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716cfeac-9c43-47cc-e3b0-99eaa925dc2f"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rY18rkng_c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9c3321-92c4-4922-8e57-20633076b02a"
      },
      "source": [
        "print(X_batch[:2])\n",
        "\n",
        "print(y_batch[:2])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[42067. 42054. 22613.  1765.]\n",
            " [10205. 28911.  7910.  2248.]]\n",
            "[[ 4. 12.  3.  6.]\n",
            " [12. 12.  7. 12.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:\n",
        "\n",
        "* [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "    - Input: (∗)\n",
        "    - Output: (∗, H)\n",
        "* [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "    - Input: (N, L, H_in) или (L, N, H_in) в зависимости от значения `batch_first`\n",
        "    - Output: (N, L, D * H_out), (h_n, c_n)\\\n",
        "    D = 2 if bidirectional=True otherwise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAr7DIjN616j"
      },
      "source": [
        "$L\\times bs \\xrightarrow{\\text{nn.Embedding}} L \\times bs \\times H_{in} \\xrightarrow{\\text{nn.LSTM}} L \\times bs \\times H^*_{out}\\xrightarrow{\\text{nn.Linear}} L \\times bs \\times \\text{tagset_size}$ \n",
        "\n",
        "\\*$H_{out}$ = lstm_hidden_dim\n",
        "\n",
        "- L - sequence length\n",
        "- bs - batch size\n",
        "- $H_{in}$ - number of input features representing an object\n",
        "- $H_{out}$ - number of output features representing an object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVEHju54d68T"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        # print('emb shape:', emb.shape)\n",
        "        output, _ = self._lstm(emb)\n",
        "        out = self._out_layer(output)\n",
        "        return out"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbrxsZ2mehWB"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# accuracy\n",
        "mask = (y_batch != 0).float() # помним, что тэг 0 соотвествует слову <pad>, не учитываем <pad> в рассчете точности\n",
        "correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "total_count = mask.sum().item()\n",
        "\n",
        "#print(f'Accuracy: {correct_count / total_count:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3_DgaXv3EZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8e484b-73b3-4d4f-a9ad-0e07f6dd78ff"
      },
      "source": [
        "print(X_batch.shape, logits.shape, preds.shape)\n",
        "# 32x4 ->(embed) 32x4x100 ->(lstm) 32x4x128 ->(linear) 32x4x13\n",
        "preds[:2]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 4]) torch.Size([32, 4, 13]) torch.Size([32, 4])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5, 9, 0, 5],\n",
              "        [6, 9, 5, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9B4nuFy9d-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b994284-7f08-4e1a-c836-501f26877c99"
      },
      "source": [
        "print(y_batch.shape)\n",
        "y_batch"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 4])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4, 12,  3,  6],\n",
              "        [12, 12,  7, 12],\n",
              "        [ 7,  9, 12,  3],\n",
              "        [ 6,  1,  8,  2],\n",
              "        [12,  3,  7,  9],\n",
              "        [ 9, 12, 12,  6],\n",
              "        [ 6,  4,  9, 12],\n",
              "        [12,  4, 12,  2],\n",
              "        [ 7,  7,  3,  5],\n",
              "        [12,  6,  6,  3],\n",
              "        [ 9,  3, 12,  9],\n",
              "        [ 1, 12,  7,  5],\n",
              "        [ 3,  9,  6, 10],\n",
              "        [ 9,  3,  2,  3],\n",
              "        [ 0, 11, 12,  8],\n",
              "        [ 0,  8,  9,  3],\n",
              "        [ 0,  2,  6, 10],\n",
              "        [ 0,  7, 12,  3],\n",
              "        [ 0,  6,  7,  2],\n",
              "        [ 0, 12,  6, 12],\n",
              "        [ 0,  7,  6,  7],\n",
              "        [ 0, 12, 12,  1],\n",
              "        [ 0,  9,  3,  9],\n",
              "        [ 0,  0,  3,  0],\n",
              "        [ 0,  0,  3,  0],\n",
              "        [ 0,  0,  3,  0],\n",
              "        [ 0,  0,  1,  0],\n",
              "        [ 0,  0,  8,  0],\n",
              "        [ 0,  0,  6,  0],\n",
              "        [ 0,  0,  2,  0],\n",
              "        [ 0,  0, 12,  0],\n",
              "        [ 0,  0,  9,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnFo9lilTw52"
      },
      "source": [
        "[nn.CrossEnropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=ignore_index)\n",
        "\n",
        "$LogLoss = -\\sum_{i=1}^{C} t_i log(p_i)$\n",
        "\n",
        "$\\ell(x, y)=L=\\left\\{l_{1}, \\ldots, l_{N}\\right\\}^{\\top}, \\quad l_{n}=-w_{y_{n}} \\log \\dfrac{\\exp \\left(x_{n, y_{n}}\\right)}{\\sum_{c=1}^{C} \\exp \\left(x_{n, c}\\right)} \\cdot 1\\left\\{y_{n} \\neq\\right. \\text { ignore_index }\\}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMUyUm1hgpe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12060b76-c260-4a3a-95c2-2e7377a4e5b9"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # при расчете Loss не учитывается 0-tag\n",
        "logits = model(X_batch)\n",
        "loss = 0\n",
        "for ind, row in enumerate(logits):\n",
        "    loss += criterion(row, y_batch[ind])  # Input: (N, C) & (N) where C - classes #\n",
        "print(loss)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(81.5645, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FprPQ0gllo7b"
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = 0\n",
        "                for ind, row in enumerate(logits):\n",
        "                    loss += criterion(row, y_batch[ind])\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                mask = (y_batch != 0).float()\n",
        "                \n",
        "                cur_correct_count, cur_sum_count = ((preds == y_batch).float() * mask).sum().item(), mask.sum().item()\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqfbeh1ltEYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f12092-46ad-4c5b-cb63-e270f2171b3d"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=5,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 5] Train: Loss = 49.84052, Accuracy = 73.79%: 100%|██████████| 572/572 [00:08<00:00, 65.33it/s]\n",
            "[1 / 5]   Val: Loss = 45.82435, Accuracy = 83.66%: 100%|██████████| 13/13 [00:00<00:00, 62.12it/s]\n",
            "[2 / 5] Train: Loss = 23.49648, Accuracy = 87.07%: 100%|██████████| 572/572 [00:07<00:00, 74.03it/s]\n",
            "[2 / 5]   Val: Loss = 33.13837, Accuracy = 88.25%: 100%|██████████| 13/13 [00:00<00:00, 59.98it/s]\n",
            "[3 / 5] Train: Loss = 16.03217, Accuracy = 91.24%: 100%|██████████| 572/572 [00:07<00:00, 74.76it/s]\n",
            "[3 / 5]   Val: Loss = 24.51983, Accuracy = 90.85%: 100%|██████████| 13/13 [00:00<00:00, 62.22it/s]\n",
            "[4 / 5] Train: Loss = 11.77118, Accuracy = 93.37%: 100%|██████████| 572/572 [00:07<00:00, 74.77it/s]\n",
            "[4 / 5]   Val: Loss = 22.83931, Accuracy = 92.01%: 100%|██████████| 13/13 [00:00<00:00, 64.26it/s]\n",
            "[5 / 5] Train: Loss = 8.84468, Accuracy = 94.71%: 100%|██████████| 572/572 [00:07<00:00, 74.86it/s]\n",
            "[5 / 5]   Val: Loss = 18.57362, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 63.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98wr38_rw55D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ceff12-eff8-4b9f-edb6-11128bb7184d"
      },
      "source": [
        "def compute_accuracy(model, data, batch_size=64):\n",
        "    model.eval()\n",
        "    val_accuracy = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "        logits = model(X_batch)\n",
        "        \n",
        "        pred = torch.argmax(logits, dim=-1)\n",
        "        mask = (y_batch != 0).float()\n",
        "        \n",
        "        correct += ((pred == y_batch).float() * mask).sum().item()\n",
        "        \n",
        "        total += mask.sum().item()        \n",
        "        \n",
        "    val_accuracy = float(correct)/total\n",
        "        \n",
        "    return val_accuracy\n",
        "\n",
        "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
        "print(f'Test accuracy is {test_ac:.2%}')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is 93.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM.\n",
        "\n",
        "Вспомним, что Unidirectional LSTM выглядела так:\n",
        "```python\n",
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        # print('emb shape:', emb.shape)\n",
        "        output, _ = self._lstm(emb)\n",
        "        out = self._out_layer(output)\n",
        "        return out\n",
        "```\n",
        "\n",
        "$L\\times bs \\xrightarrow{\\text{nn.Embedding}} L \\times bs \\times H_{in} \\xrightarrow{\\text{nn.LSTM}} L \\times bs \\times D\\cdot H_{out}\\xrightarrow{\\text{nn.Linear}} L \\times bs \\times \\text{tagset_size}$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lWBE0sYXUMz"
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, \n",
        "                             lstm_hidden_dim, \n",
        "                             num_layers=lstm_layers_count,\n",
        "                             bidirectional = True)\n",
        "        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        output, _ = self._lstm(emb)\n",
        "        out = self._out_layer(output)\n",
        "        return out"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB8o9Dx3hJ_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7566c8-2243-476a-a055-c7e57cda0007"
      },
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    lstm_layers_count=2,\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 5e-4)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), \n",
        "    epochs_count=6, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 6] Train: Loss = 21.77118, Accuracy = 88.55%: 100%|██████████| 572/572 [00:10<00:00, 56.35it/s]\n",
            "[1 / 6]   Val: Loss = 17.17040, Accuracy = 94.16%: 100%|██████████| 13/13 [00:00<00:00, 25.04it/s]\n",
            "[2 / 6] Train: Loss = 6.44454, Accuracy = 96.43%: 100%|██████████| 572/572 [00:10<00:00, 56.98it/s]\n",
            "[2 / 6]   Val: Loss = 17.29786, Accuracy = 94.89%: 100%|██████████| 13/13 [00:00<00:00, 24.15it/s]\n",
            "[3 / 6] Train: Loss = 3.99467, Accuracy = 97.41%: 100%|██████████| 572/572 [00:10<00:00, 56.90it/s]\n",
            "[3 / 6]   Val: Loss = 18.39407, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 24.10it/s]\n",
            "[4 / 6] Train: Loss = 3.17534, Accuracy = 97.85%: 100%|██████████| 572/572 [00:10<00:00, 55.16it/s]\n",
            "[4 / 6]   Val: Loss = 19.52398, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 23.47it/s]\n",
            "[5 / 6] Train: Loss = 2.93482, Accuracy = 97.99%: 100%|██████████| 572/572 [00:10<00:00, 56.47it/s]\n",
            "[5 / 6]   Val: Loss = 15.13318, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 24.61it/s]\n",
            "[6 / 6] Train: Loss = 2.71938, Accuracy = 98.13%: 100%|██████████| 572/572 [00:10<00:00, 56.07it/s]\n",
            "[6 / 6]   Val: Loss = 14.02016, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 24.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f8ed54-4407-492c-ef1a-f412d4a77545"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyHohi1GgXMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d5f633e-3e44-40eb-8380-f7932fe99a3e"
      },
      "source": [
        "w2v_model.vectors.shape\n",
        "w2v_model.get_vector('is').shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6Pko77VjFQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12992aab-26bb-46d1-e07a-71fa3e4067ef"
      },
      "source": [
        "w2v_model"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.keyedvectors.KeyedVectors at 0x7f25e3726910>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWMMQI986XJ8",
        "outputId": "a0034b68-e46b-4b7b-93fb-2824a148095c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21481ef-07fc-4fb3-fa81-24e491d44e8d"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.key_to_index:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "44NeuHTVqqk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A9O54U8jaxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131fcd26-35f7-439a-bd21-f26ae6390654"
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45441, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OMsebzNjSuX"
      },
      "source": [
        "embeddings_t = torch.from_numpy(embeddings)\n",
        "embeddings_t.requires_grad = True\n",
        "embeddings_t = embeddings_t.float().cuda()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxaRBpQd0pat"
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding.from_pretrained(embeddings_t) # важно: по дефолту freeze=True, embeddings не обучаются!\n",
        "        self._lstm = nn.LSTM(embeddings_t.shape[1], \n",
        "                             lstm_hidden_dim, \n",
        "                             num_layers=lstm_layers_count, \n",
        "                             bidirectional = True)\n",
        "        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        output, _ = self._lstm(emb)\n",
        "        out = self._out_layer(output)\n",
        "        return out"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e8356d-e5ac-4b01-87ae-eef832dd8f2a"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=35,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 35] Train: Loss = 41.93188, Accuracy = 79.92%: 100%|██████████| 572/572 [00:07<00:00, 78.14it/s]\n",
            "[1 / 35]   Val: Loss = 33.87751, Accuracy = 89.72%: 100%|██████████| 13/13 [00:00<00:00, 58.82it/s]\n",
            "[2 / 35] Train: Loss = 16.39590, Accuracy = 91.90%: 100%|██████████| 572/572 [00:07<00:00, 77.49it/s]\n",
            "[2 / 35]   Val: Loss = 25.02798, Accuracy = 92.38%: 100%|██████████| 13/13 [00:00<00:00, 58.54it/s]\n",
            "[3 / 35] Train: Loss = 12.21005, Accuracy = 93.80%: 100%|██████████| 572/572 [00:07<00:00, 76.82it/s]\n",
            "[3 / 35]   Val: Loss = 18.37932, Accuracy = 93.69%: 100%|██████████| 13/13 [00:00<00:00, 58.79it/s]\n",
            "[4 / 35] Train: Loss = 9.89550, Accuracy = 94.83%: 100%|██████████| 572/572 [00:07<00:00, 78.51it/s]\n",
            "[4 / 35]   Val: Loss = 16.20309, Accuracy = 94.51%: 100%|██████████| 13/13 [00:00<00:00, 58.27it/s]\n",
            "[5 / 35] Train: Loss = 8.45197, Accuracy = 95.49%: 100%|██████████| 572/572 [00:07<00:00, 78.59it/s]\n",
            "[5 / 35]   Val: Loss = 13.65545, Accuracy = 94.94%: 100%|██████████| 13/13 [00:00<00:00, 62.15it/s]\n",
            "[6 / 35] Train: Loss = 7.12631, Accuracy = 95.94%: 100%|██████████| 572/572 [00:07<00:00, 79.18it/s]\n",
            "[6 / 35]   Val: Loss = 14.52044, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 56.65it/s]\n",
            "[7 / 35] Train: Loss = 6.32306, Accuracy = 96.31%: 100%|██████████| 572/572 [00:07<00:00, 78.79it/s]\n",
            "[7 / 35]   Val: Loss = 11.30487, Accuracy = 95.62%: 100%|██████████| 13/13 [00:00<00:00, 57.91it/s]\n",
            "[8 / 35] Train: Loss = 5.72566, Accuracy = 96.56%: 100%|██████████| 572/572 [00:07<00:00, 77.47it/s]\n",
            "[8 / 35]   Val: Loss = 12.17446, Accuracy = 95.82%: 100%|██████████| 13/13 [00:00<00:00, 56.45it/s]\n",
            "[9 / 35] Train: Loss = 5.04640, Accuracy = 96.81%: 100%|██████████| 572/572 [00:07<00:00, 77.37it/s]\n",
            "[9 / 35]   Val: Loss = 10.83017, Accuracy = 95.96%: 100%|██████████| 13/13 [00:00<00:00, 61.01it/s]\n",
            "[10 / 35] Train: Loss = 4.57744, Accuracy = 97.00%: 100%|██████████| 572/572 [00:07<00:00, 78.06it/s]\n",
            "[10 / 35]   Val: Loss = 11.06982, Accuracy = 96.14%: 100%|██████████| 13/13 [00:00<00:00, 56.29it/s]\n",
            "[11 / 35] Train: Loss = 4.26415, Accuracy = 97.15%: 100%|██████████| 572/572 [00:07<00:00, 78.19it/s]\n",
            "[11 / 35]   Val: Loss = 10.86561, Accuracy = 96.26%: 100%|██████████| 13/13 [00:00<00:00, 58.83it/s]\n",
            "[12 / 35] Train: Loss = 3.86943, Accuracy = 97.30%: 100%|██████████| 572/572 [00:07<00:00, 78.63it/s]\n",
            "[12 / 35]   Val: Loss = 10.87510, Accuracy = 96.26%: 100%|██████████| 13/13 [00:00<00:00, 59.39it/s]\n",
            "[13 / 35] Train: Loss = 3.63877, Accuracy = 97.42%: 100%|██████████| 572/572 [00:07<00:00, 78.98it/s]\n",
            "[13 / 35]   Val: Loss = 11.35928, Accuracy = 96.36%: 100%|██████████| 13/13 [00:00<00:00, 59.30it/s]\n",
            "[14 / 35] Train: Loss = 3.40826, Accuracy = 97.54%: 100%|██████████| 572/572 [00:07<00:00, 78.84it/s]\n",
            "[14 / 35]   Val: Loss = 10.21200, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 55.44it/s]\n",
            "[15 / 35] Train: Loss = 3.17794, Accuracy = 97.64%: 100%|██████████| 572/572 [00:07<00:00, 77.49it/s]\n",
            "[15 / 35]   Val: Loss = 9.63117, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 58.67it/s]\n",
            "[16 / 35] Train: Loss = 2.99682, Accuracy = 97.71%: 100%|██████████| 572/572 [00:07<00:00, 78.44it/s]\n",
            "[16 / 35]   Val: Loss = 10.36870, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 57.25it/s]\n",
            "[17 / 35] Train: Loss = 2.80415, Accuracy = 97.82%: 100%|██████████| 572/572 [00:07<00:00, 78.70it/s]\n",
            "[17 / 35]   Val: Loss = 9.77835, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 57.10it/s]\n",
            "[18 / 35] Train: Loss = 2.64779, Accuracy = 97.90%: 100%|██████████| 572/572 [00:07<00:00, 77.93it/s]\n",
            "[18 / 35]   Val: Loss = 10.98439, Accuracy = 96.61%: 100%|██████████| 13/13 [00:00<00:00, 57.27it/s]\n",
            "[19 / 35] Train: Loss = 2.49850, Accuracy = 97.99%: 100%|██████████| 572/572 [00:07<00:00, 78.04it/s]\n",
            "[19 / 35]   Val: Loss = 11.04025, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 59.54it/s]\n",
            "[20 / 35] Train: Loss = 2.36681, Accuracy = 98.04%: 100%|██████████| 572/572 [00:07<00:00, 77.86it/s]\n",
            "[20 / 35]   Val: Loss = 10.56954, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 56.75it/s]\n",
            "[21 / 35] Train: Loss = 2.25229, Accuracy = 98.13%: 100%|██████████| 572/572 [00:07<00:00, 77.78it/s]\n",
            "[21 / 35]   Val: Loss = 11.27892, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 56.52it/s]\n",
            "[22 / 35] Train: Loss = 2.13506, Accuracy = 98.18%: 100%|██████████| 572/572 [00:07<00:00, 77.36it/s]\n",
            "[22 / 35]   Val: Loss = 11.01504, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 57.40it/s]\n",
            "[23 / 35] Train: Loss = 2.03676, Accuracy = 98.25%: 100%|██████████| 572/572 [00:07<00:00, 79.26it/s]\n",
            "[23 / 35]   Val: Loss = 11.10704, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 56.29it/s]\n",
            "[24 / 35] Train: Loss = 1.93722, Accuracy = 98.32%: 100%|██████████| 572/572 [00:07<00:00, 78.62it/s]\n",
            "[24 / 35]   Val: Loss = 10.91012, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 58.36it/s]\n",
            "[25 / 35] Train: Loss = 1.87029, Accuracy = 98.36%: 100%|██████████| 572/572 [00:07<00:00, 78.63it/s]\n",
            "[25 / 35]   Val: Loss = 9.55870, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 62.72it/s]\n",
            "[26 / 35] Train: Loss = 1.74915, Accuracy = 98.43%: 100%|██████████| 572/572 [00:07<00:00, 78.34it/s]\n",
            "[26 / 35]   Val: Loss = 10.05131, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 58.11it/s]\n",
            "[27 / 35] Train: Loss = 1.65599, Accuracy = 98.49%: 100%|██████████| 572/572 [00:07<00:00, 78.06it/s]\n",
            "[27 / 35]   Val: Loss = 10.33586, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 57.53it/s]\n",
            "[28 / 35] Train: Loss = 1.58432, Accuracy = 98.55%: 100%|██████████| 572/572 [00:07<00:00, 77.04it/s]\n",
            "[28 / 35]   Val: Loss = 9.38142, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 58.07it/s]\n",
            "[29 / 35] Train: Loss = 1.52636, Accuracy = 98.60%: 100%|██████████| 572/572 [00:07<00:00, 78.04it/s]\n",
            "[29 / 35]   Val: Loss = 11.02354, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 58.74it/s]\n",
            "[30 / 35] Train: Loss = 1.46000, Accuracy = 98.65%: 100%|██████████| 572/572 [00:07<00:00, 76.62it/s]\n",
            "[30 / 35]   Val: Loss = 10.31329, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 54.59it/s]\n",
            "[31 / 35] Train: Loss = 1.38538, Accuracy = 98.70%: 100%|██████████| 572/572 [00:07<00:00, 77.44it/s]\n",
            "[31 / 35]   Val: Loss = 11.51751, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 56.23it/s]\n",
            "[32 / 35] Train: Loss = 1.32956, Accuracy = 98.75%: 100%|██████████| 572/572 [00:07<00:00, 76.87it/s]\n",
            "[32 / 35]   Val: Loss = 11.42721, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 56.01it/s]\n",
            "[33 / 35] Train: Loss = 1.25819, Accuracy = 98.80%: 100%|██████████| 572/572 [00:07<00:00, 76.24it/s]\n",
            "[33 / 35]   Val: Loss = 10.56903, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 57.06it/s]\n",
            "[34 / 35] Train: Loss = 1.19928, Accuracy = 98.85%: 100%|██████████| 572/572 [00:07<00:00, 77.27it/s]\n",
            "[34 / 35]   Val: Loss = 10.80923, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 57.67it/s]\n",
            "[35 / 35] Train: Loss = 1.15147, Accuracy = 98.89%: 100%|██████████| 572/572 [00:07<00:00, 76.61it/s]\n",
            "[35 / 35]   Val: Loss = 11.50078, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 56.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74840cc2-b4f2-4790-da5c-b396ea45b291"
      },
      "source": [
        "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
        "print(f'Test accuracy is {test_ac:.2%}')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is 96.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTjnRRF42lJH"
      },
      "source": [
        "### Сравнение результатов работы разных подходов\n",
        "| №      | Название        | Точность\n",
        "| :---:  |:-------------:  | :------------------ | \n",
        "| 1      | Unigram Tagger  | 92.62%              | \n",
        "| 2      | Bigram Tagger   | 93.42%              | \n",
        "| 3      | Trigram Tagger  | 93.28%              | \n",
        "| 4      | LSTM            | 94.02%              | \n",
        "| 5      | BiLSTM          | 95.42%              | \n",
        "| 6      | BiLSTM Pretrained embeddings| 96.82%  |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A7Km9BYmAgC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}